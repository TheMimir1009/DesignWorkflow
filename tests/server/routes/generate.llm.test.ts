/**
 * Generate Routes LLM Integration Tests
 * Tests for LLM provider selection in document generation
 */
import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import express from 'express';
import request from 'supertest';
import { generateRouter, setClaudeCodeRunner } from '../../../server/routes/generate';
import * as llmSettingsStorage from '../../../server/utils/llmSettingsStorage';
import * as llmProvider from '../../../server/utils/llmProvider';
import type { ProjectLLMSettings } from '../../../src/types/llm';
import { createDefaultProjectLLMSettings, createDefaultModelConfig } from '../../../src/types/llm';

// Create test app
const app = express();
app.use(express.json());
app.use('/api/generate', generateRouter);

describe('Generate Routes - LLM Provider Selection', () => {
  // Mock Claude Code runner (default fallback)
  const mockClaudeCodeRunner = vi.fn().mockResolvedValue({
    output: 'Generated content from Claude Code',
    rawOutput: 'Raw output',
  });

  beforeEach(() => {
    vi.clearAllMocks();
    setClaudeCodeRunner(mockClaudeCodeRunner);
  });

  afterEach(() => {
    vi.restoreAllMocks();
  });

  describe('design-document endpoint with LLM selection', () => {
    it('should use default Claude Code when no project settings exist', async () => {
      // Mock no settings found
      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(
        createDefaultProjectLLMSettings('test-project')
      );

      const response = await request(app)
        .post('/api/generate/design-document')
        .send({
          qaResponses: [{ question: 'Test?', answer: 'Answer' }],
          projectId: 'test-project',
        });

      expect(response.status).toBe(200);
      expect(response.body.success).toBe(true);
      // Default provider is Claude Code
      expect(mockClaudeCodeRunner).toHaveBeenCalled();
    });

    it('should use configured provider for design stage', async () => {
      // Create settings with OpenAI configured for designDoc
      const settings: ProjectLLMSettings = {
        ...createDefaultProjectLLMSettings('test-project'),
        providers: [
          {
            provider: 'openai',
            apiKey: 'sk-test-key',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'gemini',
            apiKey: '',
            isEnabled: false,
            connectionStatus: 'untested',
          },
          {
            provider: 'claude-code',
            apiKey: '',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'lmstudio',
            apiKey: '',
            endpoint: 'http://localhost:1234/v1',
            isEnabled: false,
            connectionStatus: 'untested',
          },
        ],
        taskStageConfig: {
          designDoc: createDefaultModelConfig('openai', 'gpt-4o'),
          prd: null,
          prototype: null,
          defaultModel: createDefaultModelConfig('claude-code', 'claude-3.5-sonnet'),
        },
      };

      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(settings);

      // Mock the LLM provider
      const mockGenerate = vi.fn().mockResolvedValue({
        success: true,
        content: 'Generated by OpenAI',
        provider: 'openai',
        model: 'gpt-4o',
      });

      vi.spyOn(llmProvider, 'createLLMProvider').mockReturnValue({
        provider: 'openai',
        generate: mockGenerate,
        testConnection: vi.fn(),
        getAvailableModels: vi.fn(),
      });

      const response = await request(app)
        .post('/api/generate/design-document')
        .send({
          qaResponses: [{ question: 'Test?', answer: 'Answer' }],
          projectId: 'test-project',
        });

      expect(response.status).toBe(200);
      expect(response.body.success).toBe(true);
      expect(llmProvider.createLLMProvider).toHaveBeenCalled();
      expect(mockGenerate).toHaveBeenCalled();
    });

    it('should return error when configured provider is not enabled', async () => {
      // Create settings with OpenAI configured but disabled
      const settings: ProjectLLMSettings = {
        ...createDefaultProjectLLMSettings('test-project'),
        providers: [
          {
            provider: 'openai',
            apiKey: 'sk-test-key',
            isEnabled: false, // Disabled
            connectionStatus: 'connected',
          },
          {
            provider: 'claude-code',
            apiKey: '',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'gemini',
            apiKey: '',
            isEnabled: false,
            connectionStatus: 'untested',
          },
          {
            provider: 'lmstudio',
            apiKey: '',
            endpoint: 'http://localhost:1234/v1',
            isEnabled: false,
            connectionStatus: 'untested',
          },
        ],
        taskStageConfig: {
          designDoc: createDefaultModelConfig('openai', 'gpt-4o'),
          prd: null,
          prototype: null,
          defaultModel: createDefaultModelConfig('claude-code', 'claude-3.5-sonnet'),
        },
      };

      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(settings);

      const response = await request(app)
        .post('/api/generate/design-document')
        .send({
          qaResponses: [{ question: 'Test?', answer: 'Answer' }],
          projectId: 'test-project',
        });

      expect(response.status).toBe(400);
      expect(response.body.error).toContain('not enabled');
    });

    it('should return error when configured provider has no API key', async () => {
      // Create settings with OpenAI configured but no API key
      const settings: ProjectLLMSettings = {
        ...createDefaultProjectLLMSettings('test-project'),
        providers: [
          {
            provider: 'openai',
            apiKey: '', // No API key
            isEnabled: true,
            connectionStatus: 'untested',
          },
          {
            provider: 'claude-code',
            apiKey: '',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'gemini',
            apiKey: '',
            isEnabled: false,
            connectionStatus: 'untested',
          },
          {
            provider: 'lmstudio',
            apiKey: '',
            endpoint: 'http://localhost:1234/v1',
            isEnabled: false,
            connectionStatus: 'untested',
          },
        ],
        taskStageConfig: {
          designDoc: createDefaultModelConfig('openai', 'gpt-4o'),
          prd: null,
          prototype: null,
          defaultModel: createDefaultModelConfig('claude-code', 'claude-3.5-sonnet'),
        },
      };

      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(settings);

      const response = await request(app)
        .post('/api/generate/design-document')
        .send({
          qaResponses: [{ question: 'Test?', answer: 'Answer' }],
          projectId: 'test-project',
        });

      expect(response.status).toBe(400);
      expect(response.body.error).toContain('not configured');
    });
  });

  describe('prd endpoint with LLM selection', () => {
    it('should use stage-specific provider for PRD generation', async () => {
      const settings: ProjectLLMSettings = {
        ...createDefaultProjectLLMSettings('test-project'),
        providers: [
          {
            provider: 'gemini',
            apiKey: 'test-gemini-key',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'claude-code',
            apiKey: '',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'openai',
            apiKey: '',
            isEnabled: false,
            connectionStatus: 'untested',
          },
          {
            provider: 'lmstudio',
            apiKey: '',
            endpoint: 'http://localhost:1234/v1',
            isEnabled: false,
            connectionStatus: 'untested',
          },
        ],
        taskStageConfig: {
          designDoc: null,
          prd: createDefaultModelConfig('gemini', 'gemini-1.5-pro'),
          prototype: null,
          defaultModel: createDefaultModelConfig('claude-code', 'claude-3.5-sonnet'),
        },
      };

      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(settings);

      const mockGenerate = vi.fn().mockResolvedValue({
        success: true,
        content: 'Generated PRD by Gemini',
        provider: 'gemini',
        model: 'gemini-1.5-pro',
      });

      vi.spyOn(llmProvider, 'createLLMProvider').mockReturnValue({
        provider: 'gemini',
        generate: mockGenerate,
        testConnection: vi.fn(),
        getAvailableModels: vi.fn(),
      });

      const response = await request(app)
        .post('/api/generate/prd')
        .send({
          designDocContent: 'Design document content',
          projectId: 'test-project',
        });

      expect(response.status).toBe(200);
      expect(response.body.success).toBe(true);
      expect(mockGenerate).toHaveBeenCalled();
    });
  });

  describe('prototype endpoint with LLM selection', () => {
    it('should use default model when no stage config exists', async () => {
      const settings: ProjectLLMSettings = {
        ...createDefaultProjectLLMSettings('test-project'),
        taskStageConfig: {
          designDoc: null,
          prd: null,
          prototype: null, // No stage config
          defaultModel: createDefaultModelConfig('claude-code', 'claude-3.5-sonnet'),
        },
      };

      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(settings);

      const response = await request(app)
        .post('/api/generate/prototype')
        .send({
          prdContent: 'PRD content',
          projectId: 'test-project',
        });

      expect(response.status).toBe(200);
      // Default provider (Claude Code) should be used
      expect(mockClaudeCodeRunner).toHaveBeenCalled();
    });

    it('should use LMStudio when configured for prototype', async () => {
      const settings: ProjectLLMSettings = {
        ...createDefaultProjectLLMSettings('test-project'),
        providers: [
          {
            provider: 'lmstudio',
            apiKey: '',
            endpoint: 'http://localhost:1234/v1',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'claude-code',
            apiKey: '',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'openai',
            apiKey: '',
            isEnabled: false,
            connectionStatus: 'untested',
          },
          {
            provider: 'gemini',
            apiKey: '',
            isEnabled: false,
            connectionStatus: 'untested',
          },
        ],
        taskStageConfig: {
          designDoc: null,
          prd: null,
          prototype: createDefaultModelConfig('lmstudio', 'local-model'),
          defaultModel: createDefaultModelConfig('claude-code', 'claude-3.5-sonnet'),
        },
      };

      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(settings);

      const mockGenerate = vi.fn().mockResolvedValue({
        success: true,
        content: 'Generated prototype by LMStudio',
        provider: 'lmstudio',
        model: 'local-model',
      });

      vi.spyOn(llmProvider, 'createLLMProvider').mockReturnValue({
        provider: 'lmstudio',
        generate: mockGenerate,
        testConnection: vi.fn(),
        getAvailableModels: vi.fn(),
      });

      const response = await request(app)
        .post('/api/generate/prototype')
        .send({
          prdContent: 'PRD content',
          projectId: 'test-project',
        });

      expect(response.status).toBe(200);
      expect(response.body.success).toBe(true);
      expect(mockGenerate).toHaveBeenCalled();
    });
  });

  describe('error handling without fallback', () => {
    it('should return LLM error without auto-fallback', async () => {
      const settings: ProjectLLMSettings = {
        ...createDefaultProjectLLMSettings('test-project'),
        providers: [
          {
            provider: 'openai',
            apiKey: 'sk-test-key',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'claude-code',
            apiKey: '',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'gemini',
            apiKey: '',
            isEnabled: false,
            connectionStatus: 'untested',
          },
          {
            provider: 'lmstudio',
            apiKey: '',
            endpoint: 'http://localhost:1234/v1',
            isEnabled: false,
            connectionStatus: 'untested',
          },
        ],
        taskStageConfig: {
          designDoc: createDefaultModelConfig('openai', 'gpt-4o'),
          prd: null,
          prototype: null,
          defaultModel: createDefaultModelConfig('claude-code', 'claude-3.5-sonnet'),
        },
      };

      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(settings);

      const mockGenerate = vi.fn().mockResolvedValue({
        success: false,
        error: 'API rate limit exceeded',
        provider: 'openai',
        model: 'gpt-4o',
      });

      vi.spyOn(llmProvider, 'createLLMProvider').mockReturnValue({
        provider: 'openai',
        generate: mockGenerate,
        testConnection: vi.fn(),
        getAvailableModels: vi.fn(),
      });

      const response = await request(app)
        .post('/api/generate/design-document')
        .send({
          qaResponses: [{ question: 'Test?', answer: 'Answer' }],
          projectId: 'test-project',
        });

      // Should return error, not fallback to Claude Code
      expect(response.status).toBe(500);
      expect(response.body.error).toContain('LLM generation failed');
      expect(response.body.provider).toBe('openai');
      // Claude Code should NOT be called as fallback
      expect(mockClaudeCodeRunner).not.toHaveBeenCalled();
    });

    it('should include provider info in error response', async () => {
      const settings: ProjectLLMSettings = {
        ...createDefaultProjectLLMSettings('test-project'),
        providers: [
          {
            provider: 'gemini',
            apiKey: 'test-key',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'claude-code',
            apiKey: '',
            isEnabled: true,
            connectionStatus: 'connected',
          },
          {
            provider: 'openai',
            apiKey: '',
            isEnabled: false,
            connectionStatus: 'untested',
          },
          {
            provider: 'lmstudio',
            apiKey: '',
            endpoint: 'http://localhost:1234/v1',
            isEnabled: false,
            connectionStatus: 'untested',
          },
        ],
        taskStageConfig: {
          designDoc: createDefaultModelConfig('gemini', 'gemini-1.5-pro'),
          prd: null,
          prototype: null,
          defaultModel: createDefaultModelConfig('claude-code', 'claude-3.5-sonnet'),
        },
      };

      vi.spyOn(llmSettingsStorage, 'getLLMSettingsOrDefault').mockResolvedValue(settings);

      const mockGenerate = vi.fn().mockRejectedValue(new Error('Network error'));

      vi.spyOn(llmProvider, 'createLLMProvider').mockReturnValue({
        provider: 'gemini',
        generate: mockGenerate,
        testConnection: vi.fn(),
        getAvailableModels: vi.fn(),
      });

      const response = await request(app)
        .post('/api/generate/design-document')
        .send({
          qaResponses: [{ question: 'Test?', answer: 'Answer' }],
          projectId: 'test-project',
        });

      expect(response.status).toBe(500);
      expect(response.body.provider).toBe('gemini');
      expect(response.body.model).toBe('gemini-1.5-pro');
    });
  });

  describe('backward compatibility', () => {
    it('should work without projectId (use default Claude Code)', async () => {
      const response = await request(app)
        .post('/api/generate/design-document')
        .send({
          qaResponses: [{ question: 'Test?', answer: 'Answer' }],
          // No projectId
        });

      expect(response.status).toBe(200);
      expect(response.body.success).toBe(true);
      expect(mockClaudeCodeRunner).toHaveBeenCalled();
    });
  });
});
